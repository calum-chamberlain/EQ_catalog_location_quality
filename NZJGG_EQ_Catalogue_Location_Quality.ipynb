{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Plotting of Earthquake Catalogue Location Quality Scores \n",
    "\n",
    "This code is designed to accompany the manuscript Warren-Smith et al., submitted to NZJGG June 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from obspy import UTCDateTime, read_events, Catalog\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn.client import FDSNRequestTooLargeException\n",
    "\n",
    "from helpers.nz_polygons import KNOWN_REGIONS, extract_polygon\n",
    "from helpers.catalog_functions import (\n",
    "    quality_measures_check, assign_variables, min_spick_dist, \n",
    "    binary_counts)\n",
    "from helpers.plotting import (\n",
    "    plot_catalog, plot_azimuthal_map, plot_depth_scatter,\n",
    "    plot_quality_criteria_scores, plot_quality_score_map,\n",
    "    plot_quality_score_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get a catalog of interest\n",
    "We can select a more precise region in the next cell, so just get a catalog that at least encompasses your region of interest.\n",
    "Here we use a catalog from an FDSN client, but you could also read one from disk, or get one from another supplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define client used to download catalog\n",
    "client = Client(\"GEONET\")\n",
    "\n",
    "# Define start and end dates and any constraint on depth\n",
    "starttime = UTCDateTime(2018, 1, 1) \n",
    "endtime = UTCDateTime(2023, 1, 1) \n",
    "maxdepth = 50\n",
    "\n",
    "# Define an initial quadrilateral of interest if downloading from a Client, \n",
    "# which can then be filtered by a polygon region later.\n",
    "# This is currently set up for the 'West Coast' region - see below for polygon\n",
    "maxlatitude = -41.25\n",
    "minlatitude = -45\n",
    "minlongitude = 167.5\n",
    "maxlongitude = 173.5\n",
    "\n",
    "bounds = f\"{maxlatitude}-{minlatitude}_{maxlongitude}-{minlongitude}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output/input location for catalogue\n",
    "cat_out = f\"temp_files/{bounds}_{starttime.strftime('%Y')}_{endtime.strftime('%Y')}_depth{maxdepth}.xml\"\n",
    "\n",
    "# We will store temporary files in the temp_files directory.\n",
    "if not os.path.isdir(\"temp_files\"):\n",
    "    os.makedirs(\"temp_files\")\n",
    "\n",
    "#if haven't alreasy generated catalogue make it from client, or read in regional catalog if exists already\n",
    "if not os.path.exists(cat_out):\n",
    "    print(\"No existing catalogue for selection: downloading from client\")\n",
    "    fdsn_kwargs = dict(\n",
    "        maxlatitude=maxlatitude, minlatitude=minlatitude,\n",
    "        maxlongitude=maxlongitude, minlongitude=minlongitude, \n",
    "        maxdepth=maxdepth)\n",
    "    try:\n",
    "        cat = client.get_events(\n",
    "            starttime=starttime, endtime=endtime, **fdsn_kwargs)\n",
    "    except FDSNRequestTooLargeException:\n",
    "        print(\"Request too large, splitting into chunks\")\n",
    "        chunk_size = 365 * 86400\n",
    "        _starttime, _endtime = starttime, starttime + chunk_size\n",
    "        cat = Catalog()\n",
    "        while _endtime < endtime:\n",
    "            print(f\"Getting catalog between {_starttime} and {_endtime}\")\n",
    "            cat += client.get_events(\n",
    "                _starttime, _endtime, **fdsn_kwargs)\n",
    "            _starttime += chunk_size\n",
    "            _endtime += chunk_size\n",
    "        # Get the last bit\n",
    "        print(f\"Getting catalog between {_starttime} and {endtime}\")\n",
    "        cat += client.get_events(\n",
    "            _starttime, endtime, **fdsn_kwargs)\n",
    "    print(f\"Read in {len(cat)} events into cat\")\n",
    "    #cat.write(cat_out, format='QUAKEML')\n",
    "else:\n",
    "    print(\"Existing catalogue found for selection: reading in from file\")\n",
    "    cat = read_events(cat_out)\n",
    "    print(f\"Read in {len(cat)} events into cat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see what the average depth of events in the catalogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output median and mean depths of events from catalogue:\n",
    "depths = np.array([(ev.preferred_origin() or ev.origins[-1]).depth for ev in cat])\n",
    "depths /= 1000  # Convert to km\n",
    "\n",
    "print(f\"Median event depth in catalogue = {np.median(depths):.2f} km\")\n",
    "print(f\"Mean event depth in catalogue = {np.mean(depths):.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in the station inventory to get the information needed to make calculations of the quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_kwargs = dict(\n",
    "    network=\"NZ\", station=\"*\", location=\"*\",\n",
    "    starttime=starttime, endtime=endtime, level='channel')\n",
    "bbinv = client.get_stations(channel=\"HH?\", **inv_kwargs)\n",
    "spinv = client.get_stations(channel=\"EH?\", **inv_kwargs)\n",
    "sminv = client.get_stations(channel=\"BN?\", **inv_kwargs)\n",
    "sminv += client.get_stations(channel=\"HN?\", **inv_kwargs)\n",
    "\n",
    "bblons = [s.longitude for n in bbinv for s in n]\n",
    "bblats = [s.latitude for n in bbinv for s in n]\n",
    "\n",
    "splons = [s.longitude for n in spinv for s in n]\n",
    "splats = [s.latitude for n in spinv for s in n]\n",
    "\n",
    "inv = bbinv + spinv + sminv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to variability in cataloging methods, not all events have the quality measures that we need. In this cell we check all the events have quality metrics, and compute the metrics we need (azimuthal gap and minimum station distance) for any events without those metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking and calculating quality measures for in catalog')\n",
    "cat = quality_measures_check(catalog=cat, inventory=inv)\n",
    "print('Checked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Polygons and Extract Events\n",
    "\n",
    "We define specific regions known in nz_polygons, but you could define any shapely Polygon that suits you, or skip this step if you have already restricted your catalog how you want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Known regions to select from: \\n{KNOWN_REGIONS.keys()}\")\n",
    "region = 'West_Coast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define polygon based on pre-defined region in NZ_Polygons.py\n",
    "\n",
    "poly = extract_polygon(region=region)\n",
    "print(poly)\n",
    "\n",
    "print(f\"Extracting events within defined polygon for {region} region\")\n",
    "cat_poly = Catalog()\n",
    "for ev in cat:\n",
    "    x = Point(ev.preferred_origin().longitude, ev.preferred_origin().latitude)\n",
    "    if poly.contains(x):\n",
    "        cat_poly.append(ev)\n",
    "    else:\n",
    "        continue\n",
    "print(f\"Found {len(cat_poly)} events within polygon\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out polygon catalogue if it doesn't already exist\n",
    "if not os.path.exists(cat_poly_out):\n",
    "        cat_poly.write(cat_poly_out, format='QUAKEML')\n",
    "else:\n",
    "    print(f\"Reading from {cat_poly_out}\")\n",
    "    cat_poly = read_events(cat_poly_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Key Variables from Events and Plot Catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the key information about origin locations and network coverage from the catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Assigning variables')\n",
    "max_gap, depths, min_dist, mags, lats, lons, fixed = assign_variables(cat_poly=cat_poly, inventory=inv)\n",
    "print('Variables assigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Catalogue\n",
    "fig = plot_catalog(\n",
    "    region=region, lats=lats, lons=lons, \n",
    "    mags=mags, depths=depths, poly=poly, splons=splons, \n",
    "    splats=splats, bblons=bblons, bblats=bblats)\n",
    "\n",
    "#save figure\n",
    "fig.savefig(f'plots/cat_map_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/cat_map_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Azimuthal Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of all the earthquakes, coloured by their maximum azimuthal gap (red shades: gap > 180 degrees, blue shades: gap < 180 degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_azimuthal_map(lats=lats, lons=lons, max_gap=max_gap, poly=poly, region=region,\n",
    "                         bblons=bblons, bblats=bblats, splons=splons, splats=splats)\n",
    "\n",
    "\n",
    "fig.savefig(f'plots/azimuthal_constraints_map_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/azimuthal_constraints_map_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Min_Distance Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot showing the depth of each earthquake against distance to the closest picked station. Events plotting to the lower left of the solid black line have a station within one focal depth's distance. Same for dashed black line, but within 1.4* focal depth's distance. Events plotting to the right of both lines have no pick within 1.4* focal depth's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascertain if s-pick exists within 1.4*depth epicentral distance for each event\n",
    "s_picks = min_spick_dist(cat_poly=cat_poly, inventory=inv)\n",
    "\n",
    "# plot depth scatter plot\n",
    "fig = plot_depth_scatter(min_dist=min_dist, s_picks=s_picks, depths=depths, fixed=fixed, region=region)\n",
    "\n",
    "fig.savefig(f'plots/min_distance_constraints_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/min_distance_constraints_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Criteria Scores and Quality Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a bar plot showing percentages of events satisfying each of the six quality criteria. Orange = pass criteria, peach = fail criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary counts for each quality criteria\n",
    "# 1 indicates criteria satisfied, 0 indicates criteria not satisfied\n",
    "\n",
    "min_az_bi, min_picks, ps, min_dist_bi, fixed_inv = binary_counts(max_gap=max_gap, cat_poly=cat_poly, \n",
    "                                                                 depths=depths, min_dist=min_dist, fixed=fixed)\n",
    "\n",
    "# plot these up\n",
    "fig, counts, criteria = plot_quality_criteria_scores(fixed_inv=fixed_inv, cat_poly=cat_poly, s_picks=s_picks, min_dist_bi=min_dist_bi,\n",
    "                                 ps=ps, min_picks=min_picks, min_az_bi=min_az_bi, region=region)\n",
    "fig.savefig(f'plots/quality_criteria_6_hist_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/quality_criteria_6_hist_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out these percentages to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Latex\n",
    "\n",
    "# Percentages of events satisfying each criteria:\n",
    "print('Percentages of events satisfying each criteria:')\n",
    "for i, c in enumerate(criteria):\n",
    "    display(Latex(f\"{c}: {counts[i]:.1f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of scores and plot map and pie chart \n",
    "sums=[]\n",
    "counts=[]\n",
    "for i, ev in enumerate(cat_poly):\n",
    "    sums.append(min_az_bi[i] + ps[i] + min_picks[i] + fixed_inv[i] + min_dist_bi[i] + s_picks[i])\n",
    "quals=np.arange(0, 7, 1)\n",
    "for n in quals:\n",
    "    counts.append(sums.count(n))\n",
    "\n",
    "## NOTE: if depth fixing is not used in selected catalogue, a 5-QS scoring system should be employed. \n",
    "## i.e. disregard QS6 and use QS5 as the mazimum possible score    \n",
    "\n",
    "# plot map    \n",
    "fig = plot_quality_score_map(\n",
    "    quals=quals, counts=counts, lons=lons, lats=lats, region=region, poly=poly,\n",
    "    cat_poly=cat_poly, sums=sums, bblons=bblons, bblats=bblats, splons=splons, splats=splats)\n",
    "\n",
    "fig.savefig(f'plots/quality_constraints_6_map_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/quality_constraints_6_map_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the percentages for each QS and also plot as bar chart\n",
    "\n",
    "countspc, countspc_cum=[],[]\n",
    "for c in counts:\n",
    "    countspc.append(c/len(sums)*100)\n",
    "countspc_cum=np.cumsum(countspc)\n",
    "\n",
    "print('Final Quality Score Percentages for ' + region + ' region are:')\n",
    "for i, QS in enumerate(['QS0', 'QS1', 'QS2', 'QS3', 'QS4', 'QS5', 'QS6']):\n",
    "    print(QS + ': ' + str(round(countspc[i],1)))\n",
    "\n",
    "# plot as bar chart\n",
    "\n",
    "fig=plot_quality_score_bar(region=region, countspc_cum=countspc_cum, quals=quals)\n",
    "    \n",
    "fig.savefig(f'plots/quality_constraints_6_barchart_{region}_{maxdepth}.eps', dpi=800, bbox_inches='tight')\n",
    "fig.savefig(f'plots/quality_constraints_6_barchart_{region}_{maxdepth}.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write out the catalogue with the Quality Score saved within the preferred (or last) origin's comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.core.event.base import Comment\n",
    "for i, ev in enumerate(cat_poly):\n",
    "    quality_score=sums[i]\n",
    "    (ev.preferred_origin() or ev.origins[-1]).comments.append(Comment(text=f\"Quality score: {quality_score}\"))\n",
    "    \n",
    "cat_out = f\"temp_files/{region}_{starttime.strftime('%Y')}_{endtime.strftime('%Y')}_{maxdepth}_quality_scores.xml\"\n",
    "cat.write(cat_out, format='QUAKEML')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
